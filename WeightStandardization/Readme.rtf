Overview

This project demonstrates a complete, production-style machine learning workflow for a churn classification problem using intentionally simple (toy) data. The emphasis is not on predictive performance, but on understanding how real ML systems are built, evaluated, and used to support business decisions.

The project is written from the perspective of a Product / Project Manager working closely with Data Scientists, with extensive in-code explanations that translate common data science concepts into plain language and practical implications.

All work is done in standard Python scripts (no notebooks) to better reflect real-world engineering and deployment workflows.

Why I Built This

In many AI and Data Science projects, PMs are expected to:

Review and approve models they didn’t personally build

Make decisions based on probabilities, metrics, and tradeoffs

Translate technical outputs into business impact

Support deployment and iteration without fully understanding model behavior

I built this project to close the gap between managing ML projects and truly understanding them.

Specifically, I wanted to:

Learn what data scientists mean by terms like scaling, calibration, thresholds, precision, recall, and pipelines

Practice building a model end-to-end the same way a DS team would

Understand how modeling choices affect cost, workload, and risk

Create a portfolio artifact that demonstrates systems thinking, not just algorithm usage

The use of toy data is intentional. It keeps the focus on concepts, structure, and decision-making, rather than data cleaning or feature hunting.

What This Project Demonstrates
End-to-End ML Pipeline

Uses scikit-learn’s Pipeline to combine:

Feature standardization (StandardScaler)

Logistic regression classification

Guarantees that training and prediction always apply the same preprocessing

Prevents common production failures such as forgetting to scale new data

Purposeful Toy Data Design

Two input features:

Large-magnitude feature (represents meaningful behavior like usage or spend)

Small random feature (represents weak or noisy signals)

Churn labels are driven primarily by the meaningful feature

Makes it easy to validate whether the model is learning sensible patterns

Model Evaluation and Why It Matters

Each evaluation artifact answers a different business question:

ROC Curve

Measures overall ranking quality

Answers: Can the model generally separate churners from non-churners?

Precision–Recall Curve

More informative when churners are a minority

Highlights tradeoffs between catching churners and avoiding false alarms

Confusion Matrix

Shows exact counts of:

True positives (caught churners)

False positives (false alarms)

False negatives (missed churners)

Directly connects model behavior to cost and risk

Calibration Plot

Tests whether predicted probabilities can be trusted

Example question: When the model says 70% churn risk, do ~70% actually churn?

Critical for forecasting, budgeting, and intervention strategies

Threshold Tradeoff Analysis

Demonstrates how changing the probability cutoff affects:

Precision

Recall

False positive rate

Number of customers flagged (workload / cost)

Reinforces that threshold selection is a business decision, not a purely technical one

Feature Weights (Interpretability)

Visualizes standardized model weights

Confirms the model relies on meaningful features

Supports sanity checks and feature selection discussions

Production-Style Practices Included

Train/test split with stratification

Threshold tuning based on F1 score

Saving and loading:

Full preprocessing + model pipeline

Selected decision threshold

Re-predicting using loaded artifacts to ensure consistent behavior

These steps mirror how models are typically prepared for real-world deployment and review.

Project Structure
project_folder/
│
├── toy_churn_pipeline_with_calibration_and_threshold_tradeoffs.py
├── models/
│   ├── toy_churn_pipeline.joblib
│   └── threshold.json
└── README.md

How to Run

(Recommended) Create and activate a virtual environment

Install dependencies:

pip install scikit-learn numpy matplotlib joblib


Run the script:

python toy_churn_pipeline_with_calibration_and_threshold_tradeoffs.py


Review printed metrics and interactive plots

Intended Audience

Product Managers or Project Managers working on AI / Data Science initiatives

Analysts transitioning into applied machine learning

Anyone who wants to understand how ML outputs connect to real business decisions

Key Takeaway

A real machine learning system is not just an algorithm.

It is a combination of:

Data preprocessing

Model logic

Probability interpretation

Decision thresholds

Evaluation and monitoring

This project demonstrates all of those components in a clear, explainable, and portfolio-ready way using simple toy data.
